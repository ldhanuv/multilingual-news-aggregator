import nltk
import feedparser
import string
from langdetect import detect
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords



nltk.download('punkt')
nltk.download('stopwords')

hindi_stopwords = {
    'à¤¹à¥ˆ', 'à¤”à¤°', 'à¤¥à¤¾', 'à¤¥à¥‡', 'à¤•à¥€', 'à¤•à¥‡', 'à¤•à¤¾', 'à¤¸à¥‡', 'à¤•à¥‹', 'à¤ªà¤°', 'à¤¯à¤¹', 'à¤¹à¥ˆà¤‚', 'à¤¨à¤¹à¥€à¤‚',
    'à¤¤à¥‹', 'à¤­à¥€', 'à¤²à¥‡à¤•à¤¿à¤¨', 'à¤¯à¤¾', 'à¤à¤•', 'à¤®à¥‡à¤‚', 'à¤•à¤°', 'à¤°à¤¹à¤¾', 'à¤²à¤¿à¤', 'à¤¹à¥‹', 'à¤°à¤¹à¥€', 'à¤—à¤¯à¤¾',
    'à¤œà¥ˆà¤¸à¥‡', 'à¤œà¤¬', 'à¤¤à¤•', 'à¤œà¥‹', 'à¤œà¤¿à¤¸', 'à¤œà¤¿à¤¸à¥‡', 'à¤œà¤¿à¤¸à¤¨à¥‡'
}

stop_words = set(stopwords.words('english')) | hindi_stopwords

rss_urls = {
    "BBC": "http://feeds.bbci.co.uk/news/rss.xml",
    "NDTV": "https://feeds.feedburner.com/NDTV-LatestNews",
    "Navbharat Times": "https://navbharattimes.indiatimes.com/rssfeedsdefault.cms"
}

def fetch_rss_articles(url):
    feed = feedparser.parse(url)
    articles = []
    for entry in feed.entries[:5]:  # Limit to latest 5 articles
        article = {
            "title": entry.title,
            "link": entry.link,
            "published": entry.published if 'published' in entry else "N/A",
            "summary": entry.summary
        }
        articles.append(article)
    return articles

def clean_text(text):
    words = word_tokenize(text)
    cleaned_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]
    return " ".join(cleaned_words)

def detect_language(text):
    try:
        return detect(text)
    except:
        return "unknown"

def get_cleaned_news():
    final_news = []
    for source, url in rss_urls.items():
        articles = fetch_rss_articles(url)
        for article in articles:
            language = detect_language(article['summary'])
            cleaned = clean_text(article['summary'])
            final_news.append({
                "source": source,
                "title": article['title'],
                "language": language,
                "cleaned_summary": cleaned,
                "link": article['link']
            })
    return final_news



def run_tkinter_gui():
    import tkinter as tk
    from tkinter import ttk, scrolledtext

    root = tk.Tk()
    root.title("ðŸ“° Multilingual News Aggregator")
    root.geometry("800x600")

    def display_news():
        news = get_cleaned_news()
        output_box.delete(1.0, tk.END)
        for item in news:
            output_box.insert(tk.END, f"\nðŸ—žï¸ Title: {item['title']}\n")
            output_box.insert(tk.END, f"ðŸŒ Language: {item['language'].upper()}\n")
            output_box.insert(tk.END, f"ðŸ§¹ Cleaned Summary: {item['cleaned_summary']}\n")
            output_box.insert(tk.END, f"ðŸ”— Link: {item['link']}\n")
            output_box.insert(tk.END, f"Source: {item['source']}\n")
            output_box.insert(tk.END, "\n" + "-"*80 + "\n")

    fetch_button = ttk.Button(root, text="Fetch Latest News", command=display_news)
    fetch_button.pack(pady=10)

    output_box = scrolledtext.ScrolledText(root, wrap=tk.WORD, font=("Arial", 12))
    output_box.pack(expand=True, fill='both', padx=10, pady=10)

    root.mainloop()



def run_streamlit_app():
    import streamlit as st

    st.set_page_config(page_title="ðŸ“° Multilingual News Aggregator", layout="wide")
    st.title("ðŸ—žï¸ Multilingual News Aggregator (English + Hindi)")

    if st.button("Fetch News Feeds"):
        news = get_cleaned_news()
        for item in news:
            st.subheader(item['title'])
            st.markdown(f"**Source:** {item['source']} | **Language:** `{item['language'].upper()}`")
            st.write(item['cleaned_summary'])
            st.markdown(f"[Read full article]({item['link']})")
            st.markdown("---")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == 'gui':
        run_tkinter_gui()
    elif len(sys.argv) > 1 and sys.argv[1] == 'web':
        run_streamlit_app()
    else:
        print("Usage: python news_aggregator.py [gui|web]")